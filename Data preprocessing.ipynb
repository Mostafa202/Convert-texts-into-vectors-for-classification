{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e68111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "pd.options.display.max_rows = 100\n",
    "from pyarabic.araby import *\n",
    "from itertools import groupby\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "240c2007",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "TRAIN_PATH = 'preprocessed_train_data.pkl'\n",
    "VALID_PATH = 'preprocessed_valid_data.pkl'\n",
    "TEST_PATH  = 'preprocessed_test_data.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f32706",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle('fetched_dialect_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d260739a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>dialect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Nw8ieJUwaCAAreT ŸÑŸÉŸÜ ÿ®ÿßŸÑŸÜŸáÿßŸäÿ© .. ŸäŸÜÿ™ŸÅÿ∂ .. Ÿäÿ∫Ÿäÿ± .</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@7zNqXP0yrODdRjK ŸäÿπŸÜŸä Ÿáÿ∞ÿß ŸÖÿ≠ÿ≥Ÿàÿ® ÿπŸÑŸâ ÿßŸÑÿ®ÿ¥ÿ± .. ÿ≠...</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@KanaanRema ŸÖÿ®ŸäŸÜ ŸÖŸÜ ŸÉŸÑÿßŸÖŸá ÿÆŸÑŸäÿ¨Ÿä</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@HAIDER76128900 Ÿäÿ≥ŸÑŸÖŸÑŸä ŸÖÿ±Ÿàÿ±ŸÉ Ÿàÿ±Ÿàÿ≠ŸÉ ÿßŸÑÿ≠ŸÑŸàŸáüíê</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@hmo2406 ŸàŸäŸÜ ŸáŸÑ ÿßŸÑÿ∫Ÿäÿ®Ÿá  ÿßÿÆ ŸÖÿ≠ŸÖÿØ üå∏üå∫</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text dialect\n",
       "0   @Nw8ieJUwaCAAreT ŸÑŸÉŸÜ ÿ®ÿßŸÑŸÜŸáÿßŸäÿ© .. ŸäŸÜÿ™ŸÅÿ∂ .. Ÿäÿ∫Ÿäÿ± .      IQ\n",
       "1  @7zNqXP0yrODdRjK ŸäÿπŸÜŸä Ÿáÿ∞ÿß ŸÖÿ≠ÿ≥Ÿàÿ® ÿπŸÑŸâ ÿßŸÑÿ®ÿ¥ÿ± .. ÿ≠...      IQ\n",
       "2                    @KanaanRema ŸÖÿ®ŸäŸÜ ŸÖŸÜ ŸÉŸÑÿßŸÖŸá ÿÆŸÑŸäÿ¨Ÿä      IQ\n",
       "3         @HAIDER76128900 Ÿäÿ≥ŸÑŸÖŸÑŸä ŸÖÿ±Ÿàÿ±ŸÉ Ÿàÿ±Ÿàÿ≠ŸÉ ÿßŸÑÿ≠ŸÑŸàŸáüíê      IQ\n",
       "4                 @hmo2406 ŸàŸäŸÜ ŸáŸÑ ÿßŸÑÿ∫Ÿäÿ®Ÿá  ÿßÿÆ ŸÖÿ≠ŸÖÿØ üå∏üå∫      IQ"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70684cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing for dataset\n",
    "\n",
    "#removing arabic stopwords\n",
    "stop_words = stopwords.words('arabic') \n",
    "def filter_text(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to filter text\n",
    "    \n",
    "    Parameters:\n",
    "      * text(string): text that is filtered from special characters and others\n",
    "      \n",
    "    Return text(string): filtered text using regular expression\n",
    "    \"\"\"\n",
    "    #filter text by removing special characters -> not arabic words or numbers -> trim spcases -> removing stopwords and others\n",
    "    filtered_text = \" \".join([word for word in text.split(' ') if word not in stop_words])\n",
    "    filtered_text = \" \".join(re.findall('[\\u0600-\\u06ff]+',filtered_text))\n",
    "    filtered_text = re.sub('\\s+',' ',re.sub('[Ÿ†-Ÿ©ÿüÿå]','',filtered_text))\n",
    "    filtered_text = re.sub(\"[ÿ•ÿ£ÿ¢ÿß]\", \"ÿß\", filtered_text)\n",
    "    filtered_text = \"\".join(c for c, _ in groupby(filtered_text))\n",
    "    filtered_text = strip_tashkeel(filtered_text)\n",
    "    filtered_text = strip_lastharaka(filtered_text)\n",
    "    filtered_text = strip_tatweel(filtered_text)\n",
    "\n",
    "    return filtered_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c8015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy dataset\n",
    "data = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cb0f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = dataset['text'].apply(lambda x:filter_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cfe16d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Nw8ieJUwaCAAreT ŸÑŸÉŸÜ ÿ®ÿßŸÑŸÜŸáÿßŸäÿ© .. ŸäŸÜÿ™ŸÅÿ∂ .. Ÿäÿ∫Ÿäÿ± .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#before filter\n",
    "dataset['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f8f9851",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ÿ®ÿßŸÑŸÜŸáÿßŸäÿ© ŸäŸÜÿ™ŸÅÿ∂ Ÿäÿ∫Ÿäÿ±'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#after filter\n",
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee1a471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty strings\n",
    "data.drop(index=data[data['text']==''].index.to_list(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ef4235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into training validation and testing datasets and transform only training dataset into vectors using dict\n",
    "\n",
    "train_data,valid_data = train_test_split(data,test_size=0.2,random_state=0)\n",
    "valid_data,test_data = train_test_split(valid_data,test_size=0.5,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b64161e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training dataset  :  366476\n",
      "length of validation dataset:  45809\n",
      "length of testing dataset   :  45810\n"
     ]
    }
   ],
   "source": [
    "print('length of training dataset  : ',len(train_data))\n",
    "print('length of validation dataset: ',len(valid_data))\n",
    "print('length of testing dataset   : ',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22d05fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(texts,dialects):\n",
    "    \"\"\"\n",
    "    Function to build frequencies of each word in tweet according to its dialect\n",
    "    \n",
    "    Parameters:\n",
    "      * texts(pd.Series): contains all tweets\n",
    "      * dialects(pd.Series): contains all dialects\n",
    "      \n",
    "    Return vocab_dict(dict): a dictionary contains frequency of each word for each dialect\n",
    "    \"\"\"\n",
    "    vocab_dict = {}\n",
    "    for y,text in zip(dialects,texts):\n",
    "        for word in text.split(' '):\n",
    "            pair = (y,word)\n",
    "            if pair in vocab_dict:\n",
    "                vocab_dict[pair] += 1\n",
    "            else:\n",
    "                vocab_dict[pair] = 1\n",
    "                \n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f002a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_freqs(train_data['text'],train_data['dialect'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ebb063",
   "metadata": {},
   "source": [
    "* we notice that we have built vocab on only training dataset until we handle words that are out of vocab when testing, we can check the model if it works well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ba8a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text, freqs, classes, num_classes=18):\n",
    "    \"\"\"\n",
    "    Function to build a vector using frequencies of each word of each dialect\n",
    "    \n",
    "    Parameters:\n",
    "      * text(string): tweet text\n",
    "      * freqs(dict): dictionary of frequencies of words and their dialects\n",
    "      \n",
    "    Return(array): vector contains of frequencies\n",
    "    \"\"\"\n",
    "   \n",
    "    x = np.zeros((1, num_classes)) \n",
    "        \n",
    "    #loop through each word in the list of words\n",
    "    for word in text.split(' '):\n",
    "        for i in range(num_classes):\n",
    "            \n",
    "            x[0,i] += freqs.get((classes[i],word),0)\n",
    "            \n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d273a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dialects\n",
    "classes = list(np.unique(dataset['dialect']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cdeda52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 22.,  23.,  10.,  28.,  12.,  29.,  42., 205.,  23.,   2.,  15.,\n",
       "         35.,  29.,  20.,   6.,  17.,   3.,   5.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test vector\n",
    "extract_features(train_data['text'][0],vocab,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef1a12aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-c24b4fdc7188>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['text'] = train_data['text'].apply(lambda x:list(extract_features(x,vocab,classes)[0]))\n"
     ]
    }
   ],
   "source": [
    "#convert training data from text into vectors\n",
    "train_data['text'] = train_data['text'].apply(lambda x:list(extract_features(x,vocab,classes)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05597b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert validation data from text into vectors\n",
    "valid_data['text'] = valid_data['text'].apply(lambda x:list(extract_features(x,vocab,classes)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1544cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert testing data from text into vectors\n",
    "test_data['text'] = test_data['text'].apply(lambda x:list(extract_features(x,vocab,classes)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42170f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split column vector into multiple columns\n",
    "train_data_split = pd.DataFrame(train_data['text'].to_list(), columns = ['col'+str(i) for i in range(1,19)])\n",
    "train_data_split['dialect'] = train_data['dialect'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75e7c0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split column vector into multiple columns\n",
    "valid_data_split = pd.DataFrame(valid_data['text'].to_list(), columns = ['col'+str(i) for i in range(1,19)])\n",
    "valid_data_split['dialect'] = valid_data['dialect'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "997a451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split column vector into multiple columns\n",
    "test_data_split = pd.DataFrame(test_data['text'].to_list(), columns = ['col'+str(i) for i in range(1,19)])\n",
    "test_data_split['dialect'] = test_data['dialect'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1bf07c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "      <th>col6</th>\n",
       "      <th>col7</th>\n",
       "      <th>col8</th>\n",
       "      <th>col9</th>\n",
       "      <th>col10</th>\n",
       "      <th>col11</th>\n",
       "      <th>col12</th>\n",
       "      <th>col13</th>\n",
       "      <th>col14</th>\n",
       "      <th>col15</th>\n",
       "      <th>col16</th>\n",
       "      <th>col17</th>\n",
       "      <th>col18</th>\n",
       "      <th>dialect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>604.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>6442.0</td>\n",
       "      <td>554.0</td>\n",
       "      <td>691.0</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>589.0</td>\n",
       "      <td>1066.0</td>\n",
       "      <td>465.0</td>\n",
       "      <td>467.0</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>671.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>2721.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>EG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>SY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>195.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>722.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>508.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>LY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5048.0</td>\n",
       "      <td>4078.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>12296.0</td>\n",
       "      <td>1644.0</td>\n",
       "      <td>4083.0</td>\n",
       "      <td>8301.0</td>\n",
       "      <td>2405.0</td>\n",
       "      <td>6862.0</td>\n",
       "      <td>1295.0</td>\n",
       "      <td>2848.0</td>\n",
       "      <td>6594.0</td>\n",
       "      <td>6054.0</td>\n",
       "      <td>5108.0</td>\n",
       "      <td>965.0</td>\n",
       "      <td>1878.0</td>\n",
       "      <td>1942.0</td>\n",
       "      <td>1371.0</td>\n",
       "      <td>EG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4171.0</td>\n",
       "      <td>2828.0</td>\n",
       "      <td>2908.0</td>\n",
       "      <td>6596.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>3872.0</td>\n",
       "      <td>4608.0</td>\n",
       "      <td>3378.0</td>\n",
       "      <td>4507.0</td>\n",
       "      <td>1859.0</td>\n",
       "      <td>1181.0</td>\n",
       "      <td>2954.0</td>\n",
       "      <td>3586.0</td>\n",
       "      <td>2412.0</td>\n",
       "      <td>1139.0</td>\n",
       "      <td>1624.0</td>\n",
       "      <td>3044.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     col1    col2    col3     col4    col5    col6    col7    col8    col9  \\\n",
       "0   604.0   607.0   583.0   6442.0   554.0   691.0  1060.0   589.0  1066.0   \n",
       "1    21.0    50.0   138.0    494.0    15.0   381.0    67.0    96.0    96.0   \n",
       "2   195.0   202.0    99.0    722.0    81.0   277.0   304.0   242.0   419.0   \n",
       "3  5048.0  4078.0  1280.0  12296.0  1644.0  4083.0  8301.0  2405.0  6862.0   \n",
       "4  4171.0  2828.0  2908.0   6596.0   253.0  3872.0  4608.0  3378.0  4507.0   \n",
       "\n",
       "    col10   col11   col12   col13   col14   col15   col16   col17   col18  \\\n",
       "0   465.0   467.0  1328.0   671.0   509.0  2721.0   224.0   425.0   259.0   \n",
       "1   162.0    14.0   619.0    25.0    41.0    24.0    43.0    69.0    39.0   \n",
       "2    84.0   136.0   508.0   244.0   157.0    77.0   117.0    73.0    85.0   \n",
       "3  1295.0  2848.0  6594.0  6054.0  5108.0   965.0  1878.0  1942.0  1371.0   \n",
       "4  1859.0  1181.0  2954.0  3586.0  2412.0  1139.0  1624.0  3044.0   624.0   \n",
       "\n",
       "  dialect  \n",
       "0      EG  \n",
       "1      SY  \n",
       "2      LY  \n",
       "3      EG  \n",
       "4      MA  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "973a9c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save datasets into pickle files to be used in the model\n",
    "train_data_split.to_pickle(TRAIN_PATH)\n",
    "valid_data_split.to_pickle(VALID_PATH)\n",
    "test_data_split.to_pickle(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb810e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
